# å¤šæ¨¡æ€å¤§æ¨¡å‹å¾®è°ƒ
# è¿™æ˜¯å…¬å…±å¼€å‘ç¯å¢ƒï¼Œåˆ é™¤æ–‡ä»¶æ—¶ï¼Œè¯·ç¡®è®¤æ¸…é™¤åˆ«åˆ é”™äº†
- è·¯å¾„ï¼š/home/tangou/FTMLLM
- pythonç¯å¢ƒï¼šconda activate mllm

- Qwen2.5VL
- llamafactory: https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md
- gitç‰ˆæœ¬ï¼šhttps://github.com/hiyouga/LLaMA-Factory/tree/142fd7e7558fa9c169ce3d07a3fdff3eafa7b8d7
- ubuntu:22.04
- cuda:12.4.1
- python:3.10.16



```bash
source /data/tools/setproxy.sh
git clone https://github.com/floritange/FTMLLM.git
conda create -n mllm python=3.10.16
conda activate mllm
cd /home/tangou/tangou1/FTMLLM
pip install -r requirements_llamafactory.txt

# è¯„ä¼°ï¼šhttps://blog.csdn.net/H66778899/article/details/140525340

# ä¸‹è½½æ•°æ®é›†ï¼šhttps://huggingface.co/datasets/openbmb/RLHF-V-Dataset/tree/main
# ç²¾ç®€ç‰ˆï¼šhttps://huggingface.co/datasets/llamafactory/RLHF-V/tree/main
# openbmb/RLHF-V-Dataset
# llamafactory/RLHF-V
# æ•™ç¨‹ï¼šhttps://developer.aliyun.com/article/1643200
# æ•°æ®é›†ï¼šhttps://huggingface.co/datasets/UCSC-VLAA/MedTrinity-25M

# æ•™ç¨‹æœ€æ–°: https://github.com/datawhalechina/self-llm/blob/master/models/Qwen2-VL/04-Qwen2-VL-2B%20Lora%20%E5%BE%AE%E8%B0%83.md
# ğŸ”¥https://baaidata.csdn.net/67bd31f13b685529b7ffd73c.html
# ğŸ”¥https://blog.csdn.net/2301_80247435/article/details/143678295

# ç³»åˆ—æ•™ç¨‹ï¼šhttps://zhuanlan.zhihu.com/p/26993872051
# https://blog.csdn.net/2301_80247435/article/details/143678295

=== python
from datasets import load_dataset
# åŠ è½½æ•°æ®é›† huggingface token
ds = load_dataset("UCSC-VLAA/MedTrinity-25M", "25M_demo", cache_dir="/data/huggingface/hub",token="xxxx")
===




huggingface-cli login
# token: xxxx
# huggingface-cli download --resume-download <xxx> --local-dir-use-symlinks False
export HUGGINGFACE_HUB_CACHE="/data/huggingface/hub"
huggingface-cli download --resume-download --repo-type dataset llamafactory/RLHF-V --local-dir-use-symlinks False

# æ•™ç¨‹ï¼šhttps://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md
# https://github.com/hiyouga/LLaMA-Factory/tree/main/examples
# modelä½ç½®ï¼š/data/huggingface/hub/models--Qwen--Qwen2.5-VL-32B-Instruct/snapshots/6bcf1c9155874e6961bcf82792681b4f4421d2f7
# /data/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/319ccfdc6cd974fab8373cb598dfe77ad93dedd3
# rlhfä½ç½®ï¼š/data/huggingface/hub/datasets--openbmb--RLHF-V-Dataset/snapshots/1d8e9804b59e9da64ad7b1e17d505869ab9b2ad3
# rlhfç²¾ç®€ç‰ˆï¼š/data/huggingface/hub/datasets--llamafactory--RLHF-V/snapshots/7e91ceac7cd540381751d434c8ab40ea1138ef9f
# 1.å‚è€ƒé…ç½®dataset_info.jsonæ–‡ä»¶ï¼šhttps://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md
# 2.é…ç½®qwen2.5vl_lora_dpo.yamlæ–‡ä»¶
export HUGGINGFACE_HUB_CACHE="/data/huggingface/hub"
export NCCL_P2P_LEVEL=NVL
NCCL_P2P_LEVEL=NVL HUGGINGFACE_HUB_CACHE="/data/huggingface/hub" FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=4,5,6,7 llamafactory-cli train /home/tangou/tangou1/FTMLLM/configs/qwen2.5vl_lora_sft_3.yaml
NCCL_P2P_LEVEL=NVL HUGGINGFACE_HUB_CACHE="/data/huggingface/hub" FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=4,5,6,7 llamafactory-cli chat /home/tangou/tangou1/FTMLLM/configs/qwen2.5vl_lora_sft_3_inference.yaml
NCCL_P2P_LEVEL=NVL HUGGINGFACE_HUB_CACHE="/data/huggingface/hub" FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=4,5,6,7 llamafactory-cli webchat /home/tangou/tangou1/FTMLLM/configs/qwen2.5vl_lora_sft_3_inference.yaml
kill -9 1966685 1968824 1968825 1968826 1968827 1968839 1968840 1968841


####### æ•™ç¨‹
source /data/tools/setproxy.sh
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
git checkout 142fd7e7558fa9c169ce3d07a3fdff3eafa7b8d7
conda create -n mllm python=3.10.16
conda activate mllm
# æ–°å»ºä¸ªrequirements_llamafactory.txtï¼Œé…ç½®åœ¨ä¸Šé¢
pip install -r requirements_llamafactory.txt
wget https://github.com/floritange/FTMLLM/blob/main/EETQ-1.0.1-cp310-cp310-linux_x86_64.whl
pip install EETQ-1.0.1-cp310-cp310-linux_x86_64.whl


# æ•™ç¨‹ï¼šhttps://github.com/hiyouga/LLaMA-Factory/tree/main/examples
# å‚è€ƒsftè„šæœ¬ï¼Œå¦‚ï¼š
# llamafactory-cli train examples/train_lora/qwen2vl_lora_sft.yaml
# llamafactory-cli webchat examples/inference/llama3_lora_sft.yaml
# CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export HUGGINGFACE_HUB_CACHE="/data/huggingface/hub"
# è®­ç»ƒ
NCCL_P2P_LEVEL=NVL HUGGINGFACE_HUB_CACHE="/data/huggingface/hub" FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=4,5,6,7 llamafactory-cli train /home/tangou/tangou1/FTMLLM/configs/qwen2.5vl_lora_sft_3.yaml
# æ¨ç†
NCCL_P2P_LEVEL=NVL HUGGINGFACE_HUB_CACHE="/data/huggingface/hub" FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=4,5,6,7 llamafactory-cli webchat /home/tangou/tangou1/FTMLLM/configs/qwen2.5vl_lora_sft_3_inference.yaml

#####  ä¸¤ä¸ªyamlé…ç½®å¦‚ä¸‹ï¼š

# qwen2.5vl_lora_sft_3.yaml
===
### modelï¼Œæ¨¡å‹åœ¨å…±äº«ç›®å½•ä¸‹
model_name_or_path: /data/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/319ccfdc6cd974fab8373cb598dfe77ad93dedd3
image_max_pixels: 262144
video_max_pixels: 16384
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 8
lora_target: all

### dataset
dataset: mllm_demo
dataset_dir: /home/tangou/tangou1/FTMLLM/LLaMA-Factory/data  # dataset_info.jsonæ‰€åœ¨æ–‡ä»¶å¤¹
template: qwen2_vl
cutoff_len: 2048
max_samples: 1000
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: /home/tangou/tangou1/1results/qwen2.5_vl/lora/sft_dev
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true
save_only_model: false

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

### eval
# val_size: 0.1
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 500

### SwanLab é…ç½® (è¿™é‡Œæ”¹ä¸ºè‡ªå·±çš„ä¿¡æ¯)
use_swanlab: true
swanlab_project: llamafactory
swanlab_mode: cloud
swanlab_api_key: iiuay9Y1iDp0wrrYDX4cW

# deepspeed
deepspeed: /home/tangou/tangou1/cache/ds_z3_config.json
===

# qwen2.5vl_lora_sft_3_inference.yaml
===
model_name_or_path: /data/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/319ccfdc6cd974fab8373cb598dfe77ad93dedd3
adapter_name_or_path: /home/tangou/tangou1/1results/qwen2.5_vl/lora/sft_dev
template: qwen2_vl
infer_backend: vllm  # choices: [huggingface, vllm]
trust_remote_code: true
===

# ds_z3_config.json
===
{
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto",
  "gradient_accumulation_steps": "auto",
  "gradient_clipping": "auto",
  "zero_allow_untested_optimizer": true,
  "fp16": {
    "enabled": "auto",
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "initial_scale_power": 16,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "bf16": {
    "enabled": "auto"
  },
  "zero_optimization": {
    "stage": 3,
    "overlap_comm": true,
    "contiguous_gradients": true,
    "sub_group_size": 1000000000.0,
    "reduce_bucket_size": "auto",
    "stage3_prefetch_bucket_size": "auto",
    "stage3_param_persistence_threshold": "auto",
    "stage3_max_live_parameters": 1000000000.0,
    "stage3_max_reuse_distance": 1000000000.0,
    "stage3_gather_16bit_weights_on_model_save": true
  }
}
===


  "rlhf_v": {
    "hf_hub_url": "llamafactory/RLHF-V",
    "ranking": true,
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "chosen": "chosen",
      "rejected": "rejected",
      "images": "images"
    }
  },

  "mllm_demo": {
    "file_name": "mllm_demo.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "images": "images"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },

```
```json
,
  "rlhf_v_custom": {
    "hf_hub_url": "llamafactory/RLHF-V",
    "ranking": true,
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "chosen": "chosen",
      "rejected": "rejected",
      "images": "images"
    }
  },
  "mllm_demo_custom": {
    "file_name": "mllm_demo.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "images": "images"
    }
  }
```




æ•°æ®æ ¼å¼
```json
[
  {
   
    "messages": [
      {
   
        "content": "<image>ä»–ä»¬æ˜¯è°ï¼Ÿ",
        "role": "user"
      },
      {
   
        "content": "ä»–ä»¬æ˜¯æ‹œä»æ…•å°¼é»‘çš„å‡¯æ©å’Œæ ¼é›·èŒ¨å¡ã€‚",
        "role": "assistant"
      },
      {
   
        "content": "ä»–ä»¬åœ¨åšä»€ä¹ˆï¼Ÿ",
        "role": "user"
      },
      {
   
        "content": "ä»–ä»¬åœ¨è¶³çƒåœºä¸Šåº†ç¥ã€‚",
        "role": "assistant"
      }
    ],
    "images": [
      "mllm_demo_data/1.jpg"
    ]
  }
]
```

```bash
llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path /data/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/319ccfdc6cd974fab8373cb598dfe77ad93dedd3 \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --template qwen2_vl \
    --flash_attn auto \
    --dataset_dir data \
    --dataset mllm,mllm_demo_custom \
    --cutoff_len 2048 \
    --learning_rate 5e-05 \
    --num_train_epochs 3.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-31-15-32-55 \
    --bf16 True \
    --plot_loss True \
    --trust_remote_code True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --optim adamw_torch \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all \
    --deepspeed cache/ds_z3_config.json
    --use_swanlab True \
    --swanlab_project llamafactory \
    --swanlab_api_key iiuay9Y1iDp0wrrYDX4cW \
    --swanlab_mode cloud
```




cp -r /home/tangou/tangou1/FTMLLM/share/* /data/3e/share/

```bash
.
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ ft_models		#å¾®è°ƒåçš„æ¨¡å‹
â”‚   â””â”€â”€ llm_models	    #å¾®è°ƒå‰åŸºåº§llm
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ src					#æ‰€æœ‰ä»£ç æ–‡ä»¶ä½ç½®
    â””â”€â”€ down_llm.py
```


### å¾®è°ƒï¼šhttps://github.com/hiyouga/LLaMA-Factory/tree/main/examples
- Swanlabï¼šhttps://swanlab.cn/login
- 13056500789
- tg123456
- api_key: iiuay9Y1iDp0wrrYDX4cW


```bash
export HF_ENDPOINT=https://hf-mirror.com
export USE_MODELSCOPE_HUB=1
pip install EETQ-1.0.1-cp310-cp310-linux_x86_64.whl




# æ¨èå¯è§†åŒ–ç•Œé¢
cd /home/tangou/FTMLLM/LLaMA-Factory
conda activate mllm
llamafactory-cli webui
### uié…ç½®è§ä¸‹å›¾
è¿è¡Œåçš„é…ç½®ç¼“å­˜è·¯å¾„ï¼š/home/tangou/FTMLLM/LLaMA-Factory/saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-12-17-06-35



# ========
## dataé…ç½®è·¯å¾„ï¼š/home/tangou/FTMLLM/LLaMA-Factory/dataã€‚åªéœ€è¦æŠŠæ•°æ®é›†æ”¾å…¥é…ç½®data_infoã€‚https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md
## å‚ç…§
/home/tangou/FTMLLM/LLaMA-Factory/data/llamafactory/RLHF-V # huggingfaceä¸Šé¢ä¸‹è½½
/home/tangou/FTMLLM/LLaMA-Factory/data/data_info.json
  "rlhf_v": {
    "hf_hub_url": "llamafactory/RLHF-V",
    "ranking": true,
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "chosen": "chosen",
      "rejected": "rejected",
      "images": "images"
    }


# ç»ˆç«¯
# llamafactory-cli train /home/tangou/FTMLLM/configs/qwen2.5vl_lora_dpo.yaml
# llamafactory-cli chat /home/tangou/FTMLLM/configs/qwen2.5vl_lora_dpo.yaml
# llamafactory-cli export /home/tangou/FTMLLM/configs/qwen2.5vl_lora_dpo.yaml
```


å¯è§†åŒ–
llamafactory
![alt text](images/1.png)

swanlab
![alt text](images/2.png)

åå°
![alt text](images/3.png)


webchat
![alt text](./images/4.png)

![alt text](./images/5.png)

![alt text](./images/6.png)



# æ•™ç¨‹è¯¦ç»†ç‰ˆ
vscode
vscodeæ’ä»¶å®‰è£…ï¼šchineseã€remoteã€pythonã€pylanceã€python debuggerã€Python Environment Manager

![alt text](image-12.png)

![alt text](image-13.png)



### æœåŠ¡å™¨è¿æ¥
å¤§å®¶æŸ¥çœ‹ç¾¤æ–‡ä»¶è‡ªå·±çš„userå’Œå¯†ç 

vscodeè¿æ¥:

![alt text](./images/7.png)

1. å¤åˆ¶åˆ°æ–‡ä»¶é‡Œé¢å»
```yaml
Host 36.212.4.98
  HostName 36.212.4.98
  User tangou
```

![alt text](./images/8.png)

2. ç‚¹åˆ·æ–°ï¼Œå†æ‰“å¼€æ–‡ä»¶

![alt text](image.png)

3. è¾“å…¥å¯†ç 123456ï¼Œå›è½¦

![alt text](image-1.png)

4. è¿›å…¥

![alt text](image-2.png)

![alt text](image-7.png)

5. å›åˆ°ç¬¬2æ­¥ï¼Œå†å»å¼€ä¸€ä¸ªç›®å½•ã€‚è¿™æ˜¯å…±äº«æ¨¡å‹æ•°æ®çš„ç›®å½•

![alt text](image-4.png)

![alt text](image-5.png)


### å¾®è°ƒé¢„å¤‡
- ç¯å¢ƒå˜é‡ï¼šcondaã€ollama
1. æ‰“å¼€ç»ˆç«¯

![alt text](image-14.png)

2. å‘½ä»¤è¡Œè¿è¡Œï¼Œå¤åˆ¶å‘½ä»¤è¿‡å»å›è½¦ï¼Œè¿è¡Œã€‚ä¸‹é¢æˆªå›¾æˆ‘ä¹‹å‰è¿è¡Œè¿‡äº†ï¼Œæ²¡è¿è¡Œã€‚

```bash
cat /data/tools/setenv.sh >> ~/.bashrc
source ~/.bashrc
```
![alt text](image-15.png)

![alt text](image-16.png)

æ£€æŸ¥æ˜¯å¦è¿è¡ŒæˆåŠŸ
```bash
conda info --envs #æŸ¥çœ‹condaç¯å¢ƒ
ollama list # æŸ¥çœ‹ollamaæœ‰å“ªäº›æ¨¡å‹
ollama run bsahane/Qwen2.5-VL-7B-Instruct:Q4_K_M_benxh # è¿è¡Œollamaäº¤äº’å¼ï¼Œctrl d å–æ¶ˆ
```

![alt text](image-17.png)


- vpnï¼šå…ˆä¸ç”¨ç®¡ï¼ŒçŸ¥é“è¿™ä¸ªå°±è¡Œ
```bash
# http://127.0.0.1:18099
source /data/tools/setproxy.sh  #å¯åŠ¨vpn
source /data/tools/unsetproxy.sh  #å…³é—­vpn
```

- pythonç¯å¢ƒ

1. æ‰“å¼€ç»ˆç«¯ï¼Œè¿›å…¥æ–‡ä»¶å¤¹
```bash
cd /home/tangou/tangou2 #ä½ è‡ªå·±çš„è·¯å¾„
```

![alt text](image-18.png)

2. copyæ–‡ä»¶åˆ°ç›®å½•

```bash
cp -r /data/3e/share/* /home/tangou/tangou2/
```

copyä¹‹å

![alt text](image-19.png)

3. åˆ›å»ºç¯å¢ƒå¹¶å®‰è£…åŒ…

```bash
# å¼€vpn
source /data/tools/setproxy.sh
# tg10 æ¢æˆè‡ªå·±çš„åå­—
conda create -n tg10 python=3.10.16
# åˆ‡æ¢ç¯å¢ƒ
conda activate tg10
```

![alt text](image-20.png)

4. pipå®‰è£…åŒ…

```bash
# å¦‚æœé‡æ–°æ‰“å¼€ç»ˆç«¯ï¼Œæ²¡å¯åŠ¨ã€‚è¯·å¯åŠ¨ä¸‹ï¼Œå¼€vpnã€‚
source /data/tools/setproxy.sh
# å®‰è£…åŒ…ï¼Œç¬¬ä¸€æ¬¡è·‘æ²¡ç¼“å­˜ï¼Œè¿è¡Œæ—¶é—´ä¼šå¾ˆä¹…ï¼Œåœ¨ä¸‹æ•°æ®åŒ…
pip install -r requirements.txt
# é¢å¤–å®‰è£…è¿™ä¸ªåŒ…ï¼Œpipæºæ²¡æœ‰
pip install EETQ-1.0.1-cp310-cp310-linux_x86_64.whl
```

![alt text](image-21.png)

![alt text](image-23.png)

### å¾®è°ƒï¼Œè¿™é‡Œç”¨llamafactoryæä¾›çš„æ•°æ®

1. æ•°æ®è§£è¯»

![alt text](image-24.png)

2. å¾®è°ƒåŠ è½½æ•°æ®ï¼Œé¦–å…ˆå°†è‡ªå®šä¹‰æ•°æ®é…ç½®åˆ°dataset_info.json

![alt text](image-25.png)

3. é…ç½®æ¨¡å‹è·¯å¾„

é¦–å…ˆå›åˆ°è¿æ¥dataå…±äº«ç›®å½•ï¼Œè¿vscode

![alt text](image-26.png)

å¤åˆ¶è·¯å¾„ï¼Œæˆ‘ä»¬è¿™é‡Œå¾®è°ƒ32B

![alt text](image-28.png)

3. é…ç½®å¾®è°ƒçš„é…ç½®æ–‡ä»¶qwen2.5vl_lora_sft_3.yaml

å›åˆ°åŸæ¥çš„vscodeï¼Œå°†ä¸Šé¢å¤åˆ¶çš„modelè·¯å¾„æ”¾è¿›æ¥

![alt text](image-30.png)

4. è¿è¡Œå¾®è°ƒ

æ‰“å¼€ç»ˆç«¯

![alt text](image-31.png)

```bash
# å¦‚æœé‡æ–°æ‰“å¼€ç»ˆç«¯ï¼Œæ²¡å¯åŠ¨ã€‚è¯·å¯åŠ¨ä¸‹ï¼Œå¼€vpnã€‚
source /data/tools/setproxy.sh
# åˆ‡æ¢ä½ çš„pythonç¯å¢ƒ
conda activate tg10
# è®­ç»ƒ
NCCL_P2P_LEVEL=NVL HUGGINGFACE_HUB_CACHE="/data/huggingface/hub" FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 llamafactory-cli train qwen2.5vl_lora_sft_3.yaml



# æŸ¥çœ‹è¿è¡Œè®°å½•ï¼Œswanlogæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœç«¯å£è¢«å ç”¨ï¼Œåˆ™--port xxx
conda activate tg10
swanlab watch swanlog --port 5092
```

![alt text](image-32.png)

![alt text](image-34.png)

![alt text](image-35.png)

![alt text](image-36.png)

æœ¬åœ°æµè§ˆå™¨è®¿é—®ï¼šhttp://127.0.0.1:5092

![alt text](image-37.png)


- å®æµ‹ï¼š32B
- è®­ç»ƒï¼šåœ¨per_device_train_batch_size=1çš„æƒ…å†µä¸‹ï¼Œ32Bæ˜¾å­˜ç©ºä½™å¦‚ä¸‹ï¼Œå¦‚æœè°ƒ72Béœ€è¦ä¹˜2ï¼Œ72Bå‹‰å¼ºå¤Ÿç”¨ã€‚æ ¹æ®æ˜¾å­˜ç©ºä½™ï¼Œå¯è°ƒå¤§per_device_train_batch_size=2, 4, 6, 8ä¸ç­‰ã€‚
- è®­ç»ƒï¼š6ç»„æ•°æ®ï¼ˆæ¯ç»„2-3è½®å¯¹è¯ï¼‰ï¼Œä¸€ä¸ªepochéœ€è¦ï¼š50s-80sã€‚
- æ¨ç†ï¼šå·®ç‚¹çˆ†æ˜¾å­˜

![alt text](image-38.png)

![alt text](image-50.png)

5. æ¨ç†
```bash
source /data/tools/setproxy.sh
conda activate tg10
# å¦‚æœç«¯å£å ç”¨ï¼Œè¯·æ¢ä¸ªç«¯å£
export GRADIO_SERVER_PORT=7860
NCCL_P2P_LEVEL=NVL HUGGINGFACE_HUB_CACHE="/data/huggingface/hub" FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 llamafactory-cli webchat qwen2.5vl_lora_sft_3_inference.yaml
```

![alt text](image-41.png)

![alt text](image-42.png)

æœ¬åœ°æµè§ˆå™¨è®¿é—®ï¼šhttp://0.0.0.0:7860

![alt text](image-43.png)

æ‹¿åˆšåˆšè®­ç»ƒçš„æ•°æ®æ¥æµ‹

![alt text](image-44.png)

ä¸‹è½½åˆ°æœ¬åœ°

![alt text](image-45.png)

æ¨ç†ï¼ˆè¿™ä¸ªå›¾ç‰‡åœ¨åŸæœ¬çš„æ¨¡å‹ä¸Šå°±ä¸€ä¸ªè®­ç»ƒè¿‡ï¼‰

![alt text](image-46.png)


6. è¯„ä¼°ï¼ˆå®æµ‹ï¼š32Bè¯„ä¼°æ—¶ä¼šçˆ†æ˜¾å­˜ï¼‰

```bash
source /data/tools/setproxy.sh
conda activate tg10
NCCL_P2P_LEVEL=NVL HUGGINGFACE_HUB_CACHE="/data/huggingface/hub" FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 llamafactory-cli train qwen2.5vl_lora_sft_3_evaluation.yaml
```

![alt text](image-47.png)




### é¢å¤–
webuiè¿è¡Œ

```bash
source /data/tools/setproxy.sh
conda activate tg10
# å¦‚æœç«¯å£å ç”¨è¯·æ¢ä¸ªç«¯å£
export GRADIO_SERVER_PORT=7860
llamafactory-cli webui
```

![alt text](image-48.png)

ä¸‹è½½æ¨¡å‹ã€æ•°æ®é›†

```bash
source /data/tools/setproxy.sh
conda activate tg10
huggingface-cli login # tokenæ•™ç¨‹ï¼šhttps://blog.csdn.net/m0_52625549/article/details/134255660
----
export HUGGINGFACE_HUB_CACHE="/data/huggingface/hub"  #è®¾ç½®ç¼“å­˜è·¯å¾„ï¼Œå°±æ˜¯ä¹‹å‰çš„å…±äº«ç›®å½•
# æ•°æ®é›†
huggingface-cli download --resume-download --repo-type dataset llamafactory/RLHF-V --local-dir-use-symlinks False
# æ¨¡å‹
huggingface-cli download --resume-download Qwen/Qwen2.5-VL-7B-Instruct --local-dir-use-symlinks False
```

![alt text](image-49.png)

